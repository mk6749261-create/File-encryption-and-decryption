{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2056195,
          "sourceType": "datasetVersion",
          "datasetId": 1232095
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mk6749261-create/File-encryption-and-decryption/blob/main/Copy_of_notebooka958e451aa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "debasisdotcom_name_entity_recognition_ner_dataset_path = kagglehub.dataset_download('debasisdotcom/name-entity-recognition-ner-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "XVN3UuU9HW3M"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-08T01:43:54.873545Z",
          "iopub.execute_input": "2025-09-08T01:43:54.873805Z",
          "iopub.status.idle": "2025-09-08T01:43:57.266978Z",
          "shell.execute_reply.started": "2025-09-08T01:43:54.873784Z",
          "shell.execute_reply": "2025-09-08T01:43:57.266097Z"
        },
        "id": "oW3lQuEqHW3P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-08T01:44:12.126853Z",
          "iopub.execute_input": "2025-09-08T01:44:12.127194Z",
          "iopub.status.idle": "2025-09-08T01:44:12.131896Z",
          "shell.execute_reply.started": "2025-09-08T01:44:12.127147Z",
          "shell.execute_reply": "2025-09-08T01:44:12.130905Z"
        },
        "id": "8EhraBNXHW3P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/kaggle/input/name-entity-recognition-ner-dataset/NER dataset.csv\",encoding=\"latin1\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-08T01:50:02.134267Z",
          "iopub.execute_input": "2025-09-08T01:50:02.134682Z",
          "iopub.status.idle": "2025-09-08T01:50:02.833855Z",
          "shell.execute_reply.started": "2025-09-08T01:50:02.134653Z",
          "shell.execute_reply": "2025-09-08T01:50:02.832404Z"
        },
        "id": "V5765dKGHW3Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(50)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-08T01:58:16.586336Z",
          "iopub.execute_input": "2025-09-08T01:58:16.5867Z",
          "iopub.status.idle": "2025-09-08T01:58:16.599677Z",
          "shell.execute_reply.started": "2025-09-08T01:58:16.586662Z",
          "shell.execute_reply": "2025-09-08T01:58:16.598668Z"
        },
        "id": "micd7TdgHW3Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-08T02:00:12.364318Z",
          "iopub.execute_input": "2025-09-08T02:00:12.364632Z",
          "iopub.status.idle": "2025-09-08T02:00:12.57408Z",
          "shell.execute_reply.started": "2025-09-08T02:00:12.364608Z",
          "shell.execute_reply": "2025-09-08T02:00:12.573014Z"
        },
        "id": "imuSzuWMHW3R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentence #']=df['Sentence #'].fillna(method='ffill')"
      ],
      "metadata": {
        "trusted": true,
        "id": "HnBRWHdaHW3S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentence #'].head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7H3D1RPhHW3S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "D3Knbab5HW3S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Sentenc = df.groupby(\"Sentence #\")[\"Word\"].apply(list)\n",
        "Sentenc = Sentenc.apply(lambda words: [w.lower() for w in words])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "dwPcvw9OHW3T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Sentenc"
      ],
      "metadata": {
        "id": "7YPMZ9HPMl3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = df.groupby(\"Sentence #\")[\"Tag\"].apply(list)\n",
        "\n",
        "\n",
        "label = label.apply(lambda words: [w.upper() for w in words])\n",
        "\n",
        "display(label.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5a-rvBO6HW3T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# جمع كل التاجات في ليست واحدة علشان نعمل fit\n",
        "all_tags = [tag for doc in label for tag in doc]\n",
        "\n",
        "# نعمل LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(all_tags)\n",
        "\n",
        "# نطبق التحويل على كل جملة Tags لوحدها\n",
        "numeric_labels = [le.transform(doc).tolist() for doc in label]\n",
        "\n",
        "print(label[0])          # الأصلي\n",
        "print(numeric_labels[0]) # بعد التحويل\n"
      ],
      "metadata": {
        "id": "xba4Iz3M0-y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(translator)\n",
        "\n",
        "# تطبيق على كل جملة\n",
        "Sentenc = Sentenc.apply(lambda word_list: [remove_punctuation(word) for word in word_list])\n",
        "\n",
        "print(Sentenc.head())"
      ],
      "metadata": {
        "id": "t29BLK47MvIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.remove('not')"
      ],
      "metadata": {
        "id": "jpyIDcyZMvCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "Sentenc = Sentenc.apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "gJRTJiHcR6X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sentenc.head()"
      ],
      "metadata": {
        "id": "qfG4RD4RR6TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "fL_xKDk3R6P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(text):\n",
        "    return [lemmatizer.lemmatize(word) for word in text]\n",
        "\n",
        "Sentenc = Sentenc.apply(lemmatize_words)"
      ],
      "metadata": {
        "id": "niASjBmNR6N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sentenc.head()"
      ],
      "metadata": {
        "id": "zotmMLFsR6Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "rikmBuQZeJkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from re import T\n",
        "Tokenizer=Tokenizer(oov_token=\"<OOV>\")\n",
        "Tokenizer.fit_on_texts(Sentenc)"
      ],
      "metadata": {
        "id": "DrmYssvgUceV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index=Tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "id": "tNSDWui7UcRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from collections.abc import Sequence\n",
        "Sequences=Tokenizer.texts_to_sequences(Sentenc)"
      ],
      "metadata": {
        "id": "sv21_e9UUb9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Sequences[0:5])"
      ],
      "metadata": {
        "id": "vTYw8S3wiC7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded=pad_sequences(Sequences,maxlen=50 ,padding=\"post\",truncating='post')\n",
        "print(padded[0:5])"
      ],
      "metadata": {
        "id": "Kc2D1nVPiC5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sent, temp_sent, train_tags, temp_tags = train_test_split(\n",
        "    Sentenc, label, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "val_sent, test_sent, val_tags, test_tags = train_test_split(\n",
        "    temp_sent, temp_tags, test_size=0.5, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "r61q_jsyl8X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets seqeval\n"
      ],
      "metadata": {
        "id": "sfuCGWAekben"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n"
      ],
      "metadata": {
        "id": "hlGRrwxmiC3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict = {\"tokens\": train_sent, \"ner_tags\": train_tags}\n",
        "val_dict   = {\"tokens\": val_sent, \"ner_tags\": val_tags}\n",
        "test_dict  = {\"tokens\": test_sent, \"ner_tags\": test_tags}\n",
        "\n"
      ],
      "metadata": {
        "id": "CXJ1wZK4iC1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_dict(train_dict)\n",
        "val_dataset   = Dataset.from_dict(val_dict)\n",
        "test_dataset  = Dataset.from_dict(test_dict)\n"
      ],
      "metadata": {
        "id": "fp4Lee0ciCyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "datasets = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "})"
      ],
      "metadata": {
        "id": "8T1_mrTeiCw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(datasets)\n",
        "print(datasets[\"train\"][0])   # أول مثال في train\n"
      ],
      "metadata": {
        "id": "7E1wN2TiiCaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n"
      ],
      "metadata": {
        "id": "Az9MywnXUb3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,   # مهم عشان البيانات عندنا متقسمة tokens بالفعل\n",
        "        padding=\"max_length\",       # هنحدد طول ثابت\n",
        "        max_length=128              # ممكن تغيريه حسب بياناتك\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # بيرجع الـ word لكل sub-token\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:  # دي special tokens زي [CLS], [SEP], [PAD]\n",
        "                label_ids.append(-100)  # -100 معناها نتجاهلها في الحساب\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])  # أول sub-token ياخد label الكلمة\n",
        "            else:\n",
        "                label_ids.append(-100)  # بقية sub-tokens نتجاهلهم\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ],
      "metadata": {
        "id": "oAUgGoDPoCeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datasets import Dataset\n",
        "\n",
        "# 1) نجهز كل التاجات مرة واحدة\n",
        "all_tags = [tag for doc in label for tag in doc]\n",
        "le = LabelEncoder()\n",
        "le.fit(all_tags)\n",
        "\n",
        "# 2) نعيد بناء الـ Datasets لكن بالـ numeric labels\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"tokens\": train_sent,\n",
        "    \"ner_tags\": [le.transform(tags).tolist() for tags in train_tags]\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    \"tokens\": val_sent,\n",
        "    \"ner_tags\": [le.transform(tags).tolist() for tags in val_tags]\n",
        "})\n",
        "\n",
        "test_dataset = Dataset.from_dict({\n",
        "    \"tokens\": test_sent,\n",
        "    \"ner_tags\": [le.transform(tags).tolist() for tags in test_tags]\n",
        "})\n"
      ],
      "metadata": {
        "id": "yPhaynL9sH_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. اعملي قائمة بكل الـ labels\n",
        "unique_tags = set(tag for doc in label for tag in doc)\n",
        "tag2id = {tag: idx for idx, tag in enumerate(sorted(unique_tags))}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "\n",
        "# 2. حولي الداتا بتاعت الـ labels من النصوص إلى أرقام\n",
        "encoded_labels = [[tag2id[tag] for tag in doc] for doc in label]\n",
        "\n",
        "# 3. استخدمي encoded_labels بدل labels وانتِ بتمرريها للـ dataset\n",
        "datasets = Dataset.from_dict({\n",
        "    \"tokens\": Sentenc,   # التوكنز بعد التوكنايزر\n",
        "    \"labels\": encoded_labels\n",
        "})\n"
      ],
      "metadata": {
        "id": "z5IHMwkK1zSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = Dataset.from_dict({\n",
        "    \"tokens\":Sentenc ,\n",
        "    \"ner_tags\": encoded_labels\n",
        "})\n"
      ],
      "metadata": {
        "id": "CbmuHhZw2v6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# نفترض إن datasets الأساسي فيه tokens و ner_tags\n",
        "datasets = Dataset.from_dict({\n",
        "    \"tokens\": Sentenc,\n",
        "    \"ner_tags\": encoded_labels\n",
        "})\n",
        "\n",
        "# قسميه إلى train و test و validation\n",
        "datasets = datasets.train_test_split(test_size=0.2)\n",
        "datasets = DatasetDict({\n",
        "    \"train\": datasets[\"train\"],\n",
        "    \"validation\": datasets[\"test\"].train_test_split(test_size=0.5)[\"train\"],\n",
        "    \"test\": datasets[\"test\"].train_test_split(test_size=0.5)[\"test\"],\n",
        "})\n",
        "\n",
        "print(datasets)\n"
      ],
      "metadata": {
        "id": "R6-U4wBI5DBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n"
      ],
      "metadata": {
        "id": "nppHnP6zoTmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(tokenized_datasets[\"train\"][0])"
      ],
      "metadata": {
        "id": "xiKQAzakoTR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_datasets)\n"
      ],
      "metadata": {
        "id": "fS3BC3pb4sxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n"
      ],
      "metadata": {
        "id": "6389j5nVoTOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"bert-base-cased\",        # الموديل الأساسي\n",
        "    num_labels=len(le.classes_),  # عدد التاجات بعد الـ LabelEncoder\n",
        "    id2label={i: l for i, l in enumerate(le.classes_)},  # mapping رقم → label\n",
        "    label2id={l: i for i, l in enumerate(le.classes_)}   # mapping label → رقم\n",
        ")\n"
      ],
      "metadata": {
        "id": "XohaAfaZoTLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval evaluate\n",
        "\n",
        "import evaluate\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "\n",
        "    # تجاهل -100 (tokens اللي مش معنيّة)\n",
        "    true_predictions = [\n",
        "        [le.classes_[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [le.classes_[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "ZvVBuB_zoTI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_model\",      # مكان حفظ الموديل\n",
        "    eval_strategy=\"epoch\",   # نقيم بعد كل Epoch\n",
        "    save_strategy=\"epoch\",         # نحفظ الموديل بعد كل Epoch\n",
        "    learning_rate=2e-5,            # Learning rate مناسب لـ BERT\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,            # عدد الـ Epochs\n",
        "    weight_decay=0.01,             # لتفادي overfitting\n",
        "    logging_dir=\"./logs\",          # مكان اللوجات\n",
        "    logging_steps=50,\n",
        ")"
      ],
      "metadata": {
        "id": "_squf_VZoTGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     eval_strategy=\"epoch\",   # استخدم eval_strategy بدلاً من evaluate_during_training\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     num_train_epochs=2,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_steps=10,\n",
        "#     report_to=\"none\"  # Add this line to disable reporting to wandb\n",
        ")"
      ],
      "metadata": {
        "id": "SbbUNB8ICXZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "lE3V3BlfFdsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# small_train = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "# small_val   = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(200))\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=small_train,\n",
        "#     eval_dataset=small_val,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     tokenizer=tokenizer,\n",
        "# )\n",
        "\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "id": "vSvi43VDDK7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "id": "kMhs-cSCGe73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Q_G8m8zRGevG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}